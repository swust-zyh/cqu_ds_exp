# -*- coding: utf-8 -*-
"""sol.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WUwTkFB9B3xr5sQfZwAe7ulEP68mxCx4

## data preprocess
"""

import numpy as np
import pandas as pd

# load data
data = {}
file_name = ['202212', '202301', '202302']

for f in file_name:
  data[f] = pd.read_excel("/content/drive/MyDrive/Project/cqu_ds_exp/data/"+f+".xlsx"); # dataframe

# delete invalid column
def delete_invalid_column(invalid_cols, data):
  for invalid_col in invalid_cols:
    if invalid_col in data:
      data.drop([invalid_col], axis=1, inplace=True)

del_col = ['序号', '是否离线', '设备状态']
for f in file_name:
  delete_invalid_column(del_col, data[f])

# caculate the numeber of empty row
empty_rows_number = 0
def cal_empty_row_num(data):
  empty_rows = data[data.isna().all(axis=1)]
  return len(empty_rows)

for f in file_name:
  empty_rows_number += cal_empty_row_num(data[f])

print(empty_rows_number)

# delete empty row
for f in file_name:
  data[f] = data[f].dropna(how='all')

# caculate the number of row that has empty value
total_empty_val_row_num = 0
def cal_empty_val_row(data):
  return data.isna().any(axis=1).sum()

for f in file_name:
  total_empty_val_row_num += cal_empty_val_row(data[f])

print(total_empty_val_row_num)

# delete rows that hav empty value
for f in file_name:
  data[f] = data[f].dropna()

# concat
total_data = pd.concat([data[f] for f in file_name], axis=0)

print(total_data.dtypes)

# data type trans
total_data['测量时间'] = pd.to_datetime(total_data['测量时间'])
total_data['所属杆塔'] = total_data['所属杆塔'].astype(str)
total_data['预警等级'] = total_data['预警等级'].astype(str)
total_data['覆冰告警等级'] = total_data['覆冰告警等级'].astype(int)
print(total_data.dtypes)

# delete duplicated row
total_data.drop_duplicates(keep='last', inplace=True)

print(total_data['覆冰告警等级'].drop_duplicates())
print(total_data['预警等级'].drop_duplicates())

# group
data_62 = total_data[total_data['所属杆塔'] == '62#']
data_99 = total_data[total_data['所属杆塔'] == '99#']
print(data_62.shape[0])
print(data_99.shape[0])

# 62# is useless for prediction
print(data_62['预警等级'].drop_duplicates())
print(data_62['覆冰告警等级'].drop_duplicates())
print(data_99['预警等级'].drop_duplicates())
print(data_99['覆冰告警等级'].drop_duplicates())

def warning_level_label_func(x):
  if x == '正常':
    return '0'
  elif x == '橙色预警':
    return '1'
  elif x == '黄色预警':
    return '2'
  elif x == '红色预警':
    return '3'
  else:
    print("预警等级错误")
    return

data_62['预警等级'] = data_62['预警等级'].map(warning_level_label_func)
data_62['预警等级'] = data_62['预警等级'].astype(int)
data_99['预警等级'] = data_99['预警等级'].map(warning_level_label_func)
data_99['预警等级'] = data_99['预警等级'].astype(int)
print(data_99['预警等级'].drop_duplicates())

def monitor_label_func(x):
  if x == '62#':
    return 0
  elif x == '99#':
    return 1
  else:
    print("所属杆塔错误")
    return

data_62['所属杆塔'] = data_62['所属杆塔'].map(monitor_label_func)
data_99['所属杆塔'] = data_99['所属杆塔'].map(monitor_label_func)
data_99.dtypes

data_99_sort_by_time = data_99.sort_values(by='测量时间')
print(data_99_sort_by_time['测量时间'])
print(data_99_sort_by_time)

"""## exploratory data analysis"""

import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.pyplot import MultipleLocator # 设置刻度间隔

zh_to_en = {'气温': 'temp', '空气湿度': 'air_humidity', '风速': 'wind_speed', '风向': 'wind_direction', \
            '气压': 'air_pressure', '降雨量': 'rainfall', '覆冰厚度': 'ice_thickness', '覆冰占比': 'percentage_of_ice_cover', \
            '拉力值': 'tension_value', '辐射': 'radiation'}

# features change with time
col_id = 2
plt.figure(figsize=(20, 20))
for subplot_id in range(1, 11):
  plt.subplot(5, 2, subplot_id)
  plt.plot(data_99_sort_by_time['测量时间'], data_99_sort_by_time[data_99_sort_by_time.columns.tolist()[col_id]])
  plt.title(zh_to_en[data_99_sort_by_time.columns.tolist()[col_id]])
  col_id += 1

# delete air_pressure column by the above figure
delete_invalid_column(['气压'], data_99_sort_by_time)
data_99_sort_by_time.dtypes

# impact of single feature on warning levels
col_id = 2
plt.figure(figsize=(20, 20))
for subplot_id in range(1, 10):
  plt.subplot(3, 3, subplot_id)
  plt.scatter(data_99_sort_by_time['预警等级'], data_99_sort_by_time[data_99_sort_by_time.columns.tolist()[col_id]])
  plt.title(zh_to_en[data_99_sort_by_time.columns.tolist()[col_id]])

  # set the x-axis interval
  x_major_locator=MultipleLocator(1)
  ax=plt.gca()
  ax.xaxis.set_major_locator(x_major_locator)

  col_id += 1

# impact of a single feature on ice cover alarm levels
col_id = 2
plt.figure(figsize=(20, 20))
for subplot_id in range(1, 10):
  plt.subplot(3, 3, subplot_id)
  plt.scatter(data_99_sort_by_time['覆冰告警等级'], data_99_sort_by_time[data_99_sort_by_time.columns.tolist()[col_id]])
  plt.title(zh_to_en[data_99_sort_by_time.columns.tolist()[col_id]])
  col_id += 1

# impact of single feature on warning levels
col_id = 2
plt.figure(figsize=(20, 20))
for subplot_id in range(1, 10):
  plt.subplot(3, 3, subplot_id)
  medians = []
  for warning_level in range(0, 4):
    medians.append(data_99_sort_by_time[data_99_sort_by_time['预警等级'] == warning_level][data_99_sort_by_time.columns.tolist()[col_id]].median())
  plt.bar(range(0, 4), medians)

  # set the x-axis interval
  x_major_locator=MultipleLocator(1)
  ax=plt.gca()
  ax.xaxis.set_major_locator(x_major_locator)

  plt.title(zh_to_en[data_99_sort_by_time.columns.tolist()[col_id]])
  col_id += 1

# impact of a single feature on ice cover alarm levels
col_id = 2
plt.figure(figsize=(20, 20))
for subplot_id in range(1, 10):
  plt.subplot(3, 3, subplot_id)
  medians = []
  for warning_level in range(0, 7):
    medians.append(data_99_sort_by_time[data_99_sort_by_time['覆冰告警等级'] == warning_level][data_99_sort_by_time.columns.tolist()[col_id]].median())
  plt.bar(range(0, 7), medians)

  # set the x-axis interval
  x_major_locator=MultipleLocator(1)
  ax=plt.gca()
  ax.xaxis.set_major_locator(x_major_locator)

  plt.title(zh_to_en[data_99_sort_by_time.columns.tolist()[col_id]])
  col_id += 1

# distribution of features of different warning levels
col_id = 2
subplot_id = 1
plt.figure(figsize=(40, 40))
while subplot_id < 37:
  for warning_level in range(0, 4):
    plt.subplot(9, 4, subplot_id)
    plt.hist(data_99_sort_by_time[data_99_sort_by_time['预警等级'] == warning_level][data_99_sort_by_time.columns.tolist()[col_id]])
    plt.title(zh_to_en[data_99_sort_by_time.columns.tolist()[col_id]]+" of "+str(warning_level))
    subplot_id += 1
  col_id += 1

# distribution of features of different ice cover alarm levels
col_id = 2
subplot_id = 1
plt.figure(figsize=(80, 80))
while subplot_id < 64:
  for warning_level in range(0, 7):
    plt.subplot(9, 7, subplot_id)
    plt.hist(data_99_sort_by_time[data_99_sort_by_time['覆冰告警等级'] == warning_level][data_99_sort_by_time.columns.tolist()[col_id]])
    plt.title(zh_to_en[data_99_sort_by_time.columns.tolist()[col_id]]+" of "+str(warning_level))
    subplot_id += 1
  col_id += 1

delete_invalid_column(['气压'], data_62)
data_62.dtypes

# distribution of features of different monitoring sites
plt.figure(figsize=(80, 20))

col_id = 2
for subplot_id in range(1, 10):
  plt.subplot(2, 9, subplot_id)
  plt.hist(data_62[data_62.columns.tolist()[col_id]])
  plt.title(zh_to_en[data_62.columns.tolist()[col_id]]+" of 62#")
  col_id += 1

col_id = 2
for subplot_id in range(10, 19):
  plt.subplot(2, 9, subplot_id)
  plt.hist(data_99_sort_by_time[data_99_sort_by_time.columns.tolist()[col_id]])
  plt.title(zh_to_en[data_99_sort_by_time.columns.tolist()[col_id]]+" of 99#")
  col_id += 1

"""## modeling & evaluation
研究问题
1. 分类
  * 通过气象数据分类监测位
  * 通过气象数据预测覆冰告警等级
2. 聚类
  * 聚类天气类型
"""

import lightgbm as lgb
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report

# classify monitoring sites (all features)
data_all = pd.concat([data_62, data_99_sort_by_time], axis=0)
x = data_all.iloc[:, 2:11]
y = data_all['所属杆塔']

scaler = StandardScaler()
x_scaled = scaler.fit_transform(x)

x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.2, random_state=42)

# train lgbm model
train_data = lgb.Dataset(x_train, label=y_train)
test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)

params = {
    'objective': 'multiclass',
    'num_class': 2,
    'metric': 'multi_logloss',
    'boosting_type': 'gbdt',
    'learning_rate': 0.1,
    'num_leaves': 31,
    'feature_fraction': 0.9
}

num_round = 100
bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])

# prediction & evaluation
y_pred_prob = bst.predict(x_test, num_iteration=bst.best_iteration)
y_pred = [list(x).index(max(x)) for x in y_pred_prob]
print(classification_report(y_test, y_pred))

# train svm model
svm_model = SVC(kernel='rbf', C=1.0)

svm_model.fit(x_train, y_train)

# prediction & evaluation
y_pred = svm_model.predict(x_test)
print(classification_report(y_test, y_pred))

# classify monitoring sites (choose features by EDA)
x = data_all.iloc[:, [3, 5, 9]]
y = data_all['所属杆塔']

scaler = StandardScaler()
x_scaled = scaler.fit_transform(x)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# train lgbm model
train_data = lgb.Dataset(x_train, label=y_train)
test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)

params = {
    'objective': 'multiclass',
    'num_class': 2,
    'metric': 'multi_logloss',
    'boosting_type': 'gbdt',
    'learning_rate': 0.1,
    'num_leaves': 31,
    'feature_fraction': 0.9
}

num_round = 100
bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])

# prediction & evaluation
y_pred_prob = bst.predict(x_test, num_iteration=bst.best_iteration)
y_pred = [list(x).index(max(x)) for x in y_pred_prob]
print(classification_report(y_test, y_pred))

# train svm model
svm_model = SVC(kernel='rbf', C=1.0)

svm_model.fit(x_train, y_train)

# predict & evaluation
y_pred = svm_model.predict(x_test)
print(classification_report(y_test, y_pred))

import tensorflow as tf
from sklearn.ensemble import RandomForestClassifier

# classify warning level (all features)
x = data_all.iloc[:, 2:11]
y = data_all['覆冰告警等级']

scaler = StandardScaler()
x_scaled = scaler.fit_transform(x)

x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.2, random_state=42)

# build DNN and train
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(x_train.shape[1],)),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(7, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test))

# prediction & evaluation
y_pred_prob = model.predict(x_test)
y_pred = [tf.argmax(x).numpy() for x in y_pred_prob]
print(classification_report(y_test, y_pred))

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(x_train, y_train)

y_pred = rf_model.predict(x_test)
print(classification_report(y_test, y_pred))

from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# cluster data
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_all.iloc[:, 2:11])

# sunny cloudy rainy
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(data_scaled)

labels = kmeans.labels_

np.set_printoptions(threshold=np.inf)
print(labels)

silhouette_avg = silhouette_score(data_scaled, labels) # -1 ~ 1; the closer to 1, the better
print(silhouette_avg)
calinski_harabasz_score_value = calinski_harabasz_score(data_scaled, labels)
print(calinski_harabasz_score_value)
davies_bouldin_score_value = davies_bouldin_score(data_scaled, labels)
print(davies_bouldin_score_value)

agg_clustering = AgglomerativeClustering(n_clusters=3)
agg_labels = agg_clustering.fit_predict(data_scaled)

np.set_printoptions(threshold=np.inf)
print(agg_labels)

silhouette_avg = silhouette_score(data_scaled, agg_labels) # -1 ~ 1; the closer to 1, the better
print(silhouette_avg)
calinski_harabasz_score_value = calinski_harabasz_score(data_scaled, agg_labels) # the bigger, the better
print(calinski_harabasz_score_value)
davies_bouldin_score_value = davies_bouldin_score(data_scaled, agg_labels) # the smaller, the better
print(davies_bouldin_score_value)

def unify_label(x):
  if x == 1:
    return 2
  elif x == 2:
    return 1
  else:
    return 0

for i in range(0, len(labels)):
  labels[i] = unify_label(labels[i])

print(labels != agg_labels)

